# -*- coding: utf-8 -*-
"""MeChat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dcQeAQ_vmD1YotMbfBs83SXHA4sQKnuI
"""

import os
from typing import List, Dict, Tuple
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI
import gradio as gr



class Config:
    """Configuration for the RAG pipeline"""

    PDF_PATH = "MeChat.pdf"

    # RAG Parameters
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 300
    EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
    TOP_K_RESULTS = 6
    COLLECTION_NAME = "pdf_documents"

    # Gemini Model
    GEMINI_MODEL = "models/gemini-2.5-flash"

class PDFProcessor:
    """Handle PDF loading and text chunking"""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

    def load_and_split(self, pdf_path: str) -> Tuple[List, Dict]:
        """Load PDF and split into chunks"""
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        chunks = self.text_splitter.split_documents(documents)

        stats = {
            'filename': os.path.basename(pdf_path),
            'total_pages': len(documents),
            'total_chunks': len(chunks),
            'avg_chunk_size': sum(len(chunk.page_content) for chunk in chunks) // len(chunks) if chunks else 0
        }

        return chunks, stats

class VectorStoreManager:
    """Manage embeddings and vector store"""

    def __init__(self, embedding_model: str = Config.EMBEDDING_MODEL):
        print("ðŸ§® Loading embedding model...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'}
        )
        self.vectorstore = None

    def create_vectorstore(self, chunks: List, collection_name: str = Config.COLLECTION_NAME):
        """Create vector store from document chunks"""
        print("ðŸ’¾ Creating vector store...")
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            collection_name=collection_name
        )
        return self.vectorstore

    def get_retriever(self, k: int = Config.TOP_K_RESULTS):
        """Get retriever for similarity search"""
        return self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": k}
        )

class RAGPipeline:
    """Complete RAG pipeline"""

    def __init__(self, api_key: str, pdf_path: str):
        self.config = Config()
        self.pdf_path = pdf_path
        self.pdf_processor = PDFProcessor(
            chunk_size=self.config.CHUNK_SIZE,
            chunk_overlap=self.config.CHUNK_OVERLAP
        )
        self.vector_manager = VectorStoreManager(
            embedding_model=self.config.EMBEDDING_MODEL
        )

        # Initializing Gemini LLM
        try:
            self.llm = ChatGoogleGenerativeAI(
                model=self.config.GEMINI_MODEL,
                google_api_key=api_key,
                temperature=0.3,
                convert_system_message_to_human=True
            )
        except Exception as e:
            # Fallback to stable model
            print(f"âš ï¸ Trying alternative model: gemini-2.0-flash...")
            self.llm = ChatGoogleGenerativeAI(
                model="models/gemini-2.0-flash",
                google_api_key=api_key,
                temperature=0.3,
                convert_system_message_to_human=True
            )

        self.qa_chain = None
        self.stats = {}
        self.is_ready = False

    def initialize(self) -> str:
        """Initialize the RAG pipeline with the configured PDF"""
        try:
            print(f"ðŸ“– Loading PDF: {self.pdf_path}")

            # Check if PDF exists
            if not os.path.exists(self.pdf_path):
                return f"PDF not found at: {self.pdf_path}\n\nPlease upload your PDF to Colab or update the PDF_PATH in Config."

            # Load and chunk PDF
            chunks, self.stats = self.pdf_processor.load_and_split(self.pdf_path)
            print(f"Loaded {self.stats['total_pages']} pages, created {self.stats['total_chunks']} chunks")

            # Create vector store
            vectorstore = self.vector_manager.create_vectorstore(chunks)
            retriever = self.vector_manager.get_retriever(k=self.config.TOP_K_RESULTS)

            # Create QA chain
            print("ðŸ”— Creating QA chain with Gemini...")
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True
            )

            self.is_ready = True

            status = f"""**System Initialized Successfully!**

ðŸ“„ **Document:** {self.stats['filename']}
ðŸ“– **Pages:** {self.stats['total_pages']}
ðŸ”ª **Chunks:** {self.stats['total_chunks']}
ðŸ“Š **Avg Chunk Size:** {self.stats['avg_chunk_size']} characters
ðŸ¤– **Model:** Google Gemini {self.config.GEMINI_MODEL.upper()} (FREE!)

 Ready to answer questions about the document!"""

            print("RAG Pipeline ready!")
            return status

        except Exception as e:
            return f"Error initializing system: {str(e)}"

    def query(self, question: str) -> Tuple[str, str]:
        """Query the RAG system"""
        if not self.is_ready:
            return "System not initialized. Please check the initialization status.", ""

        if not question.strip():
            return "Please enter a question.", ""

        try:
            result = self.qa_chain({"query": question})

            answer = result['result']
            sources = result.get('source_documents', [])

            # Format sources
            sources_text = ""
            if sources:
                sources_text = f"**{len(sources)} Relevant Passages:**\n\n"
                for i, doc in enumerate(sources, 1):
                    page = doc.metadata.get('page', 'Unknown')
                    preview = doc.page_content[:250].replace('\n', ' ').strip()
                    sources_text += f"**Source {i} - Page {page}**\n{preview}...\n\n"

            return answer, sources_text

        except Exception as e:
            return f"Error: {str(e)}", ""

    def get_document_info(self) -> str:
        """Get information about the loaded document"""
        if not self.is_ready:
            return "Document not loaded yet."

        return f"""**Current Document Information:**

ðŸ“„ **Filename:** {self.stats['filename']}
ðŸ“– **Total Pages:** {self.stats['total_pages']}
ðŸ”ª **Text Chunks:** {self.stats['total_chunks']}
ðŸ“Š **Average Chunk Size:** {self.stats['avg_chunk_size']} characters
ðŸ” **Retrieved Chunks per Query:** {self.config.TOP_K_RESULTS}
ðŸ¤– **AI Model:** Google Gemini {self.config.GEMINI_MODEL}
"""

def create_gradio_interface(api_key: str, pdf_path: str) -> gr.Blocks:
    """Create Gradio chat interface"""

    # Initializing RAG pipeline
    rag = RAGPipeline(api_key=api_key, pdf_path=pdf_path)
    init_status = rag.initialize()

    def answer_question(question: str, history: List) -> Tuple[List, str]:
        """Answer question and return chat history"""
        answer, sources = rag.query(question)
        history.append((question, answer))
        return history, sources

    def get_doc_info():
        """Return document information"""
        return rag.get_document_info()

    # Creating interface
    with gr.Blocks(title="PDF RAG Chat", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # MeChat
        ### Ask questions about Cherian Mathew
        """)

        chatbot = gr.Chatbot(
            label="Conversation",
            height=400,
            show_label=True
        )

        question_input = gr.Textbox(
            label="Ask a question",
            placeholder="What is this document about?",
            lines=2
        )

        with gr.Row():
            submit_btn = gr.Button("Send", variant="primary", scale=3)
            clear_btn = gr.Button("Clear Chat", scale=1)

        sources_output = gr.Textbox(
            label="Source Passages",
            lines=10,
            interactive=False
        )

        # Event handlers
        submit_btn.click(
            fn=answer_question,
            inputs=[question_input, chatbot],
            outputs=[chatbot, sources_output]
        ).then(
            fn=lambda: "",
            outputs=[question_input]
        )

        question_input.submit(
            fn=answer_question,
            inputs=[question_input, chatbot],
            outputs=[chatbot, sources_output]
        ).then(
            fn=lambda: "",
            outputs=[question_input]
        )

        clear_btn.click(
            fn=lambda: ([], ""),
            outputs=[chatbot, sources_output]
        )

    return demo

def main():
    """Main execution function"""

    # Configuration
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")  # Replace with your Gemini API key
    PDF_PATH = Config.PDF_PATH  # Uses the path from Config
    if GOOGLE_API_KEY == "your-google-api-key-here":
        print("Please set your GOOGLE_API_KEY in the code!")
        print("Get your FREE API key at: https://makersuite.google.com/app/apikey")
        return

    print("="*80)
    print("  PDF RAG SYSTEM - POWERED BY GOOGLE GEMINI (FREE!)")
    print("="*80)
    print(f"\nðŸ“„ Configured PDF: {PDF_PATH}\n")

    # Create and launch interface
    demo = create_gradio_interface(
        api_key=GOOGLE_API_KEY,
        pdf_path=PDF_PATH
    )

    demo.launch(share=True, debug=True)

# Running the application
if __name__ == "__main__":
    main()